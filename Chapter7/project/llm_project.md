# Note for the LLM project i will be working on the

# Project Idea
- Code completion Model

## inspirations and resources
- [Let's reproduce GPT-2](https://github.com/karpathy/build-nanogpt)
- [24 game](https://github.com/zhangfaen/24GameGPT)

## Dataset
- [CodeXGLUE](https://github.com/microsoft/CodeXGLUE)
- [the-stack](https://huggingface.co/datasets/bigcode/the-stack)
- [bigcodebench](https://huggingface.co/datasets/bigcode/bigcodebench)

## My Project Plan
1. [x] Make the base architecture
2. [] Go through the forward pass (using tiny shakespeare) and get the output
2. [] Build the tokenizer and the dataloader
4. [] return the loss from the forward pass
5. [] Go through the backward pass 