# Note for the LLM project i will be working on the

# Project Idea
- Code completion Model

## inspirations and resources
- [Let's reproduce GPT-2](https://github.com/karpathy/build-nanogpt)
- [24 game](https://github.com/zhangfaen/24GameGPT)

## Dataset
- [starcoderdata](https://huggingface.co/datasets/bigcode/starcoderdata)
- [CodeXGLUE](https://github.com/microsoft/CodeXGLUE)
- [the-stack](https://huggingface.co/datasets/bigcode/the-stack)
- [bigcodebench](https://huggingface.co/datasets/bigcode/bigcodebench)

## My Project Plan
1. [x] Make the base architecture
2. [x] Go through the forward pass  and get the output
3. [x] Build the tokenizer
4. [x] return the loss from the forward pass
5. [] Go through the backward pass 

6. [] Bulid my own tokenizer for code completion model using tiktoken